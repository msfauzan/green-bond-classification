{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A6Ni_ROY7nlk",
        "outputId": "a74cb635-7cf8-4160-ab8b-9d6e47e3e39f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Klasifikasi Prospektus Obligasi - Machine Learning Approach\n",
        "\n",
        "## Solusi untuk Circular Logic Problem\n",
        "\n",
        "**Masalah Original:**\n",
        "- Y (label kelas) ditentukan dari X (keyword counts)\n",
        "- ML hanya mempelajari aturan deterministik yang sudah ada\n",
        "\n",
        "**Solusi:**\n",
        "1. **Extract fitur tambahan** dari PDF (bukan hanya keyword counts)\n",
        "2. **Gunakan clustering** untuk validasi natural grouping\n",
        "3. **Fitur engineering** yang lebih kaya untuk menemukan pola tersembunyi\n",
        "4. **Evaluasi dengan proper metrics**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.12/dist-packages (3.1.5)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.3)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.12/dist-packages (from openpyxl) (2.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n"
          ]
        }
      ],
      "source": [
        "%pip install PyPDF2 pandas scikit-learn openpyxl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'PyPDF2'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3597198256.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mPyPDF2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'PyPDF2'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# Import Libraries\n",
        "import os\n",
        "import re\n",
        "import PyPDF2\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "# ML Libraries\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import (classification_report, confusion_matrix, \n",
        "                             accuracy_score, f1_score, silhouette_score)\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"Libraries imported successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =====================================================\n",
        "# FEATURE EXTRACTION - Lebih Kaya dari Keyword Count\n",
        "# =====================================================\n",
        "\n",
        "def extract_features_from_pdf(pdf_path):\n",
        "    \"\"\"\n",
        "    Extract multiple features from PDF, not just keyword counts.\n",
        "    Features:\n",
        "    - Keyword counts (green, sustainability, sustainability-linked)\n",
        "    - Document metadata (pages, total words, avg words per page)\n",
        "    - Keyword density (normalized by document length)\n",
        "    - Presence of specific phrases\n",
        "    \"\"\"\n",
        "    features = {\n",
        "        'filename': os.path.basename(pdf_path),\n",
        "        # Keyword counts\n",
        "        'green_bond_count': 0,\n",
        "        'sustainability_bond_count': 0,\n",
        "        'sustainability_linked_count': 0,\n",
        "        # Document stats\n",
        "        'total_pages': 0,\n",
        "        'total_words': 0,\n",
        "        'avg_words_per_page': 0,\n",
        "        # Additional keywords\n",
        "        'lingkungan_count': 0,\n",
        "        'berkelanjutan_count': 0,\n",
        "        'sosial_count': 0,\n",
        "        'emisi_count': 0,\n",
        "        'karbon_count': 0,\n",
        "        'energi_terbarukan_count': 0,\n",
        "        'pencapaian_count': 0,\n",
        "        'indikator_kinerja_count': 0,\n",
        "        # Keyword densities (per 1000 words)\n",
        "        'green_density': 0,\n",
        "        'sustainability_density': 0,\n",
        "        'sustainability_linked_density': 0,\n",
        "    }\n",
        "    \n",
        "    # Keywords for each class\n",
        "    green_keywords = [\"green bond\", \"green sukuk\", \"kubl\", \n",
        "                      \"berwawasan lingkungan\", \"efek bersifat utang berwawasan lingkungan\"]\n",
        "    sustainability_keywords = [\"sustainability bond\", \"sustainability sukuk\", \n",
        "                               \"ebus keberlanjutan\", \"efek bersifat utang keberlanjutan\"]\n",
        "    sustainability_linked_keywords = [\"sustainability linked bond\", \"sustainability linked sukuk\",\n",
        "                                      \"ebus terkait keberlanjutan\", \"efek bersifat utang terkait keberlanjutan\",\n",
        "                                      \"indikator kinerja utama keberlanjutan\"]\n",
        "    \n",
        "    try:\n",
        "        with open(pdf_path, 'rb') as file:\n",
        "            pdf_reader = PyPDF2.PdfReader(file)\n",
        "            features['total_pages'] = len(pdf_reader.pages)\n",
        "            \n",
        "            full_text = \"\"\n",
        "            for page in pdf_reader.pages:\n",
        "                text = page.extract_text() or \"\"\n",
        "                full_text += text.lower() + \" \"\n",
        "            \n",
        "            # Word count\n",
        "            words = full_text.split()\n",
        "            features['total_words'] = len(words)\n",
        "            features['avg_words_per_page'] = features['total_words'] / max(features['total_pages'], 1)\n",
        "            \n",
        "            # Count keywords\n",
        "            for kw in green_keywords:\n",
        "                features['green_bond_count'] += full_text.count(kw.lower())\n",
        "            for kw in sustainability_keywords:\n",
        "                features['sustainability_bond_count'] += full_text.count(kw.lower())\n",
        "            for kw in sustainability_linked_keywords:\n",
        "                features['sustainability_linked_count'] += full_text.count(kw.lower())\n",
        "            \n",
        "            # Additional keywords\n",
        "            features['lingkungan_count'] = full_text.count('lingkungan')\n",
        "            features['berkelanjutan_count'] = full_text.count('berkelanjutan')\n",
        "            features['sosial_count'] = full_text.count('sosial')\n",
        "            features['emisi_count'] = full_text.count('emisi')\n",
        "            features['karbon_count'] = full_text.count('karbon')\n",
        "            features['energi_terbarukan_count'] = full_text.count('energi terbarukan')\n",
        "            features['pencapaian_count'] = full_text.count('pencapaian')\n",
        "            features['indikator_kinerja_count'] = full_text.count('indikator kinerja')\n",
        "            \n",
        "            # Calculate densities (per 1000 words)\n",
        "            if features['total_words'] > 0:\n",
        "                features['green_density'] = (features['green_bond_count'] / features['total_words']) * 1000\n",
        "                features['sustainability_density'] = (features['sustainability_bond_count'] / features['total_words']) * 1000\n",
        "                features['sustainability_linked_density'] = (features['sustainability_linked_count'] / features['total_words']) * 1000\n",
        "                \n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {pdf_path}: {e}\")\n",
        "    \n",
        "    return features\n",
        "\n",
        "print(\"Feature extraction function defined!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =====================================================\n",
        "# PROCESS ALL PROSPEKTUS FILES\n",
        "# =====================================================\n",
        "\n",
        "# Directory containing prospektus files\n",
        "prospektus_dir = r\"d:\\1. Important\\Work\\Bank Indonesia\\DSta-DSMF\\Web Scraping\\Prospektus\"\n",
        "\n",
        "# Extract features from all PDFs\n",
        "all_features = []\n",
        "for filename in os.listdir(prospektus_dir):\n",
        "    if filename.endswith('.pdf'):\n",
        "        pdf_path = os.path.join(prospektus_dir, filename)\n",
        "        print(f\"Processing: {filename}\")\n",
        "        features = extract_features_from_pdf(pdf_path)\n",
        "        all_features.append(features)\n",
        "\n",
        "# Create DataFrame\n",
        "df = pd.DataFrame(all_features)\n",
        "print(f\"\\n‚úÖ Processed {len(df)} documents\")\n",
        "print(f\"Features extracted: {len(df.columns)} columns\")\n",
        "df.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =====================================================\n",
        "# RULE-BASED LABELING (Sebagai Initial Label)\n",
        "# =====================================================\n",
        "\n",
        "def assign_label(row):\n",
        "    \"\"\"\n",
        "    Assign label based on keyword counts - ini adalah INITIAL label\n",
        "    yang akan kita validasi dengan clustering\n",
        "    \"\"\"\n",
        "    green = row['green_bond_count']\n",
        "    sustain = row['sustainability_bond_count']\n",
        "    linked = row['sustainability_linked_count']\n",
        "    \n",
        "    # Prioritas: sustainability_linked > sustainability > green > regular\n",
        "    if linked > 0 and linked >= sustain and linked >= green:\n",
        "        return 'sustainability_linked_bond'\n",
        "    elif sustain > 0 and sustain >= green:\n",
        "        return 'sustainability_bond'\n",
        "    elif green > 0:\n",
        "        return 'green_bond'\n",
        "    else:\n",
        "        return 'obligasi_biasa'\n",
        "\n",
        "df['initial_label'] = df.apply(assign_label, axis=1)\n",
        "\n",
        "print(\"Label distribution:\")\n",
        "print(df['initial_label'].value_counts())\n",
        "print(\"\\n\" + \"=\"*50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =====================================================\n",
        "# APPROACH 1: CLUSTERING untuk Validasi Natural Grouping\n",
        "# =====================================================\n",
        "# Clustering TIDAK memerlukan label - ini cara yang VALID untuk\n",
        "# menggunakan ML karena kita mencari pola alami dalam data\n",
        "\n",
        "# Select numeric features for clustering\n",
        "feature_cols = ['total_pages', 'total_words', 'avg_words_per_page',\n",
        "                'green_bond_count', 'sustainability_bond_count', 'sustainability_linked_count',\n",
        "                'lingkungan_count', 'berkelanjutan_count', 'sosial_count',\n",
        "                'emisi_count', 'karbon_count', 'energi_terbarukan_count',\n",
        "                'pencapaian_count', 'indikator_kinerja_count',\n",
        "                'green_density', 'sustainability_density', 'sustainability_linked_density']\n",
        "\n",
        "X = df[feature_cols].fillna(0)\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Try different number of clusters\n",
        "print(\"üîç Finding optimal number of clusters...\")\n",
        "silhouette_scores = []\n",
        "for k in range(2, 8):\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "    labels = kmeans.fit_predict(X_scaled)\n",
        "    score = silhouette_score(X_scaled, labels)\n",
        "    silhouette_scores.append((k, score))\n",
        "    print(f\"  K={k}: Silhouette Score = {score:.4f}\")\n",
        "\n",
        "# Best K\n",
        "best_k = max(silhouette_scores, key=lambda x: x[1])[0]\n",
        "print(f\"\\n‚úÖ Optimal clusters: {best_k}\")\n",
        "\n",
        "# Final clustering\n",
        "kmeans = KMeans(n_clusters=best_k, random_state=42, n_init=10)\n",
        "df['cluster'] = kmeans.fit_predict(X_scaled)\n",
        "\n",
        "print(\"\\nCluster distribution:\")\n",
        "print(df['cluster'].value_counts().sort_index())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =====================================================\n",
        "# COMPARE: Cluster vs Rule-Based Labels\n",
        "# =====================================================\n",
        "# Ini untuk melihat apakah clustering menemukan pola yang sama\n",
        "# atau berbeda dengan rule-based approach\n",
        "\n",
        "print(\"üîÑ Comparing Clustering Results vs Rule-Based Labels:\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Cross-tabulation\n",
        "crosstab = pd.crosstab(df['cluster'], df['initial_label'])\n",
        "print(\"\\nCross-tabulation (Cluster vs Initial Label):\")\n",
        "print(crosstab)\n",
        "\n",
        "# Cluster characteristics\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üìä Cluster Characteristics (Mean values):\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for cluster_id in sorted(df['cluster'].unique()):\n",
        "    cluster_data = df[df['cluster'] == cluster_id]\n",
        "    print(f\"\\nüîπ Cluster {cluster_id} ({len(cluster_data)} documents):\")\n",
        "    print(f\"   - Avg Green Bond Keywords: {cluster_data['green_bond_count'].mean():.2f}\")\n",
        "    print(f\"   - Avg Sustainability Keywords: {cluster_data['sustainability_bond_count'].mean():.2f}\")\n",
        "    print(f\"   - Avg Sustainability-Linked Keywords: {cluster_data['sustainability_linked_count'].mean():.2f}\")\n",
        "    print(f\"   - Avg Total Pages: {cluster_data['total_pages'].mean():.1f}\")\n",
        "    print(f\"   - Most common label: {cluster_data['initial_label'].mode().values[0]}\")\n",
        "    \n",
        "# Documents in each cluster\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üìã Sample documents per cluster:\")\n",
        "for cluster_id in sorted(df['cluster'].unique()):\n",
        "    print(f\"\\nCluster {cluster_id}:\")\n",
        "    sample_docs = df[df['cluster'] == cluster_id]['filename'].head(5).tolist()\n",
        "    for doc in sample_docs:\n",
        "        print(f\"  - {doc}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =====================================================\n",
        "# APPROACH 2: SUPERVISED ML dengan FITUR TAMBAHAN\n",
        "# =====================================================\n",
        "# Jika ada ground truth labels (dari validasi manual), ML bisa digunakan\n",
        "# Untuk demo, kita gunakan initial_label tapi dengan FITUR YANG BERBEDA\n",
        "# dari yang digunakan untuk membuat label\n",
        "\n",
        "# IMPORTANT: Untuk menghindari circular logic, kita bisa:\n",
        "# 1. Gunakan HANYA fitur non-keyword untuk prediksi\n",
        "# 2. Atau gunakan fitur keyword sebagai \"weak supervision\"\n",
        "\n",
        "# Filter: hanya dokumen yang terklasifikasi (bukan obligasi_biasa)\n",
        "df_classified = df[df['initial_label'] != 'obligasi_biasa'].copy()\n",
        "\n",
        "if len(df_classified) > 10:\n",
        "    print(f\"üìä Using {len(df_classified)} classified documents for supervised learning\")\n",
        "    \n",
        "    # Feature set 1: HANYA fitur non-keyword (untuk menghindari circular logic)\n",
        "    non_keyword_features = ['total_pages', 'total_words', 'avg_words_per_page',\n",
        "                           'lingkungan_count', 'berkelanjutan_count', 'sosial_count',\n",
        "                           'emisi_count', 'karbon_count', 'energi_terbarukan_count',\n",
        "                           'pencapaian_count', 'indikator_kinerja_count']\n",
        "    \n",
        "    X_non_kw = df_classified[non_keyword_features].fillna(0)\n",
        "    y = df_classified['initial_label']\n",
        "    \n",
        "    # Encode labels\n",
        "    le = LabelEncoder()\n",
        "    y_encoded = le.fit_transform(y)\n",
        "    \n",
        "    # Scale features\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X_non_kw)\n",
        "    \n",
        "    # Split data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X_scaled, y_encoded, test_size=0.3, random_state=42, stratify=y_encoded\n",
        "    )\n",
        "    \n",
        "    print(f\"Train size: {len(X_train)}, Test size: {len(X_test)}\")\n",
        "    print(f\"Classes: {le.classes_}\")\n",
        "    \n",
        "    # Train multiple models dengan Cross-Validation\n",
        "    models = {\n",
        "        'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
        "        'Decision Tree': DecisionTreeClassifier(max_depth=5, random_state=42),\n",
        "        'Random Forest': RandomForestClassifier(n_estimators=100, max_depth=5, random_state=42),\n",
        "        'SVM': SVC(kernel='rbf', random_state=42)\n",
        "    }\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"ü§ñ Model Performance with Cross-Validation:\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    for name, model in models.items():\n",
        "        # Cross-validation\n",
        "        cv_scores = cross_val_score(model, X_train, y_train, cv=3, scoring='f1_weighted')\n",
        "        \n",
        "        # Train and test\n",
        "        model.fit(X_train, y_train)\n",
        "        y_pred = model.predict(X_test)\n",
        "        \n",
        "        test_acc = accuracy_score(y_test, y_pred)\n",
        "        test_f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "        \n",
        "        print(f\"\\nüìå {name}:\")\n",
        "        print(f\"   CV F1 (train): {cv_scores.mean():.4f} ¬± {cv_scores.std():.4f}\")\n",
        "        print(f\"   Test Accuracy: {test_acc:.4f}\")\n",
        "        print(f\"   Test F1 (weighted): {test_f1:.4f}\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Not enough classified documents for supervised learning\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =====================================================\n",
        "# APPROACH 3: Anomaly Detection - Cari Dokumen yang Mungkin Salah Klasifikasi\n",
        "# =====================================================\n",
        "# ML untuk menemukan dokumen yang \"berbeda\" dari kelompoknya\n",
        "# Ini berguna untuk manual review\n",
        "\n",
        "from sklearn.ensemble import IsolationForest\n",
        "\n",
        "print(\"üîç Anomaly Detection - Finding potentially misclassified documents\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Use all features\n",
        "X_full = df[feature_cols].fillna(0)\n",
        "X_scaled_full = StandardScaler().fit_transform(X_full)\n",
        "\n",
        "# Detect anomalies\n",
        "iso_forest = IsolationForest(contamination=0.1, random_state=42)\n",
        "df['is_anomaly'] = iso_forest.fit_predict(X_scaled_full)\n",
        "\n",
        "anomalies = df[df['is_anomaly'] == -1]\n",
        "print(f\"\\n‚ö†Ô∏è Found {len(anomalies)} potentially anomalous documents:\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "for _, row in anomalies.iterrows():\n",
        "    print(f\"\\nüìÑ {row['filename']}\")\n",
        "    print(f\"   Label: {row['initial_label']}\")\n",
        "    print(f\"   Green: {row['green_bond_count']}, Sustain: {row['sustainability_bond_count']}, Linked: {row['sustainability_linked_count']}\")\n",
        "    print(f\"   Pages: {row['total_pages']}, Words: {row['total_words']}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üí° Dokumen anomali ini sebaiknya di-review manual untuk validasi\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =====================================================\n",
        "# SAVE RESULTS\n",
        "# =====================================================\n",
        "\n",
        "# Create summary DataFrame\n",
        "result_df = df[['filename', 'initial_label', 'cluster', 'is_anomaly',\n",
        "                'green_bond_count', 'sustainability_bond_count', 'sustainability_linked_count',\n",
        "                'total_pages', 'total_words']].copy()\n",
        "\n",
        "result_df['is_anomaly'] = result_df['is_anomaly'].map({1: 'Normal', -1: 'Anomaly'})\n",
        "\n",
        "# Save to Excel\n",
        "output_path = r\"d:\\1. Important\\Work\\Bank Indonesia\\DSta-DSMF\\Web Scraping\\hasil_klasifikasi_ml.xlsx\"\n",
        "result_df.to_excel(output_path, index=False)\n",
        "print(f\"‚úÖ Results saved to: {output_path}\")\n",
        "\n",
        "# Display summary\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üìä CLASSIFICATION SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\nTotal documents: {len(df)}\")\n",
        "print(f\"\\nBy Initial Label:\")\n",
        "for label, count in df['initial_label'].value_counts().items():\n",
        "    print(f\"  - {label}: {count}\")\n",
        "print(f\"\\nBy Cluster:\")\n",
        "for cluster, count in df['cluster'].value_counts().sort_index().items():\n",
        "    print(f\"  - Cluster {cluster}: {count}\")\n",
        "print(f\"\\nAnomalies detected: {len(anomalies)}\")\n",
        "\n",
        "result_df.head(20)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
